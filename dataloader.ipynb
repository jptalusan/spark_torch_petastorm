{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Dataloaders\n",
    "Dataloaders allow you to load into memory chunks of data. By using the `__getitem__` function, you can load a set of files that would be used for a single batch of model training/evaluation/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = ['JP','T','ISIS']\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(idx)\n",
    "        # print(self.img_dir, idx, self.img_labels[idx])\n",
    "        # img_path = os.path.join(self.img_dir, f\"{str(self.img_labels[idx])}.csv\")\n",
    "        img_path = \"./data/customers-2000000.csv\"\n",
    "        # image = pd.read_csv(img_path).to_dict(orient='records')\n",
    "        # label = self.img_labels[idx]\n",
    "        # print(img_path, label)\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        # if self.target_transform:\n",
    "        #     label = self.target_transform(label)\n",
    "        return {}, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CustomImageDataset(img_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(data, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a dataloader, you can repeatedly read multiple large files (in this case 300mb csvs) without the need to load them all into memory at once.\n",
    "\n",
    "The caveat is you have to divide your data into chunks prior to this. Or if your data is already partitioned, then its even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, l in train_dataloader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Parquet files\n",
    "The dataset dataloader works very well with parquet files generated by spark.\n",
    "See [this post](https://stackoverflow.com/questions/68199072/pytorch-dataloader-for-reading-a-large-parquet-csv-file)\n",
    "\n",
    "This uses dask but the idea is the same.\n",
    "```python\n",
    "# Define the Dataset class\n",
    "class UsersDataset(Dataset):\n",
    "    def __init__(self, dask_df, labels):\n",
    "        self.dask_df = dask_df\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        X_df = self.dask_df.get_partition(idx).compute()\n",
    "        X = np.row_stack([X_df])\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y = self.labels[idx]\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        sample = (X_tensor, y_tensor) \n",
    "        return sample\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
